{
  "metadata": {
    "name": "arastirmaunsw",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.types._\r\n\r\nval schema \u003d new StructType().\r\nadd(\"id\",StringType,true).\r\nadd(\"dur\",StringType,true).\r\nadd(\"proto\",StringType,true).\r\nadd(\"service\",StringType,true).\r\nadd(\"state\",StringType,true).\r\nadd(\"spkts\",StringType,true).\r\nadd(\"dpkts\",StringType,true).\r\nadd(\"sbytes\",StringType,true).\r\nadd(\"dbytes\",StringType,true).\r\nadd(\"rate\",StringType,true).\r\nadd(\"sttl\",StringType,true).\r\nadd(\"dttl\",StringType,true).\r\nadd(\"sload\",StringType,true).\r\nadd(\"dload\",StringType,true).\r\nadd(\"sloss\",StringType,true).\r\nadd(\"dloss\",StringType,true).\r\nadd(\"sinpkt\",StringType,true).\r\nadd(\"dinpkt\",StringType,true).\r\nadd(\"sjit\",StringType,true).\r\nadd(\"djit\",StringType,true).\r\nadd(\"swin\",StringType,true).\r\nadd(\"stcpb\",StringType,true).\r\nadd(\"dtcpb\",StringType,true).\r\nadd(\"dwin\",StringType,true).\r\nadd(\"tcprtt\",StringType,true).\r\nadd(\"synack\",StringType,true).\r\nadd(\"ackdat\",StringType,true).\r\nadd(\"smean\",StringType,true).\r\nadd(\"dmean\",StringType,true).\r\nadd(\"trans_depth\",StringType,true).\r\nadd(\"response_body_len\",StringType,true).\r\nadd(\"ct_srv_src\",StringType,true).\r\nadd(\"ct_state_ttl\",StringType,true).\r\nadd(\"ct_dst_ltm\",StringType,true).\r\nadd(\"ct_src_dport_ltm\",StringType,true).\r\nadd(\"ct_dst_sport_ltm\",StringType,true).\r\nadd(\"ct_dst_src_ltm\",StringType,true).\r\nadd(\"is_ftp_login\",StringType,true).\r\nadd(\"ct_ftp_cmd\",StringType,true).\r\nadd(\"ct_flw_http_mthd\",StringType,true).\r\nadd(\"ct_src_ltm\",StringType,true).\r\nadd(\"ct_srv_dst\",StringType,true).\r\nadd(\"is_sm_ips_ports\",StringType,true).\r\nadd(\"attack_cat\",StringType,true).\r\nadd(\"label\",StringType,true)\r\n\r\nvar df \u003d spark.read.options(Map(\"sep\"-\u003e\",\", \"header\"-\u003e \"true\")).\r\n                schema(schema).\r\n                csv(\"/unsw_tra.csv\")\r\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "var df_num \u003d df.select(\"state\",\"Dttl\",\"synack\",\"swin\",\"dwin\",\"ct_state_ttl\",\"ct_src_ltm\",\"ct_srv_dst\",\"Sttl\",\"ct_dst_sport_ltm\",\"Djit\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df_num.show(10)"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\n\nval strIndexer \u003d new StringIndexer()\n  .setInputCol(\"state\")\n  .setOutputCol(\"indexedState\")\n  .fit(df_num)\n\ndf_num \u003d strIndexer.transform(df_num)\ndf_num \u003d df_num.drop(\"state\")"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val num_categories \u003d df_num.dtypes.filter (_._2 \u003d\u003d \"StringType\") map (_._1)\n\n\nnum_categories.foreach{r\u003d\u003e\n    df_num \u003d df_num.withColumn(r,df_num(r).cast(\"double\"))\n}"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "var df_label \u003d df.select(\"label\")"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df_label \u003d df_label.withColumn(\"label\",df_label(\"label\").cast(\"integer\"))"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "df_label.printSchema"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val df1 \u003d df_num.withColumn(\"row_id\", monotonically_increasing_id())\n\nval df2 \u003d df_label.withColumn(\"row_id\", monotonically_increasing_id())\n\nvar df_final \u003d df1.join(df2, (\"row_id\")).drop(\"row_id\")\ndf_num.drop(\"row_id\")\ndf_label.drop(\"row_id\")"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\n\nval assembler \u003d new VectorAssembler()\n  .setInputCols(df_num.columns)\n  .setOutputCol(\"features\")\n  \ndf_final \u003d assembler.transform(df_final)"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.feature.StandardScaler\n\nval scaler \u003d new StandardScaler()\n  .setInputCol(\"features\")\n  .setOutputCol(\"scaledFeatures\")\n  .setWithStd(true)\n  .setWithMean(false)\n  .fit(df_final)\n  \ndf_final \u003d scaler.transform(df_final)"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n\nval Array(training, test) \u003d df_final.randomSplit(Array(0.7, 0.3), seed \u003d 12345)"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.classification.LogisticRegression\r\n\r\nval lr \u003d new LogisticRegression()\r\n  .setLabelCol(\"label\")\r\n  .setFeaturesCol(\"features\")\r\n  .setMaxIter(10)\r\n  .setRegParam(0.4)\r\n  .setFamily(\"multinomial\")\r\n \r\nval model \u003d lr.fit(training)\r\nval predict_train\u003dmodel.transform(training)\r\nval predict_test\u003dmodel.transform(test)"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val check \u003d predict_test.withColumn(\"correct\",when(col(\"label\").equalTo(col(\"prediction\")),1).otherwise(0))"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "check.groupBy(\"correct\").count.show"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.classification.DecisionTreeClassificationModel\r\nimport org.apache.spark.ml.classification.DecisionTreeClassifier\r\n\r\nval dt \u003d new DecisionTreeClassifier()\r\n  .setLabelCol(\"label\")\r\n  .setFeaturesCol(\"features\")\r\n\r\n// Train model. This also runs the indexers.\r\nval model_dt \u003d dt.fit(training)\r\n\r\n// Make predictions.\r\nval predictions \u003d model_dt.transform(test)"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val check2 \u003d predictions.withColumn(\"correct\",when(col(\"label\").equalTo(col(\"prediction\")),1).otherwise(0))\ncheck2.groupBy(\"correct\").count.show"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.ml.Pipeline\r\nimport org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\r\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\r\nimport org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorIndexer}\r\n\r\nval rf \u003d new RandomForestClassifier()\r\n  .setLabelCol(\"label\")\r\n  .setFeaturesCol(\"features\")\r\n  .setNumTrees(10)\r\n  \r\nval model_rf \u003d rf.fit(training)"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val predictions_rf \u003d model_rf.transform(test)"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val check3 \u003d predictions_rf.withColumn(\"correct\",when(col(\"label\").equalTo(col(\"prediction\")),1).otherwise(0))\ncheck3.groupBy(\"correct\").count.show"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "check3.select(\"label\",\"prediction\").write.format(\"csv\").save(\"rf.csv\")"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "check2.select(\"label\",\"prediction\").write.format(\"csv\").save(\"dt.csv\")"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "check.select(\"label\",\"prediction\").write.format(\"csv\").save(\"lr.csv\")"
    }
  ]
}